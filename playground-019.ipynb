{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpmIiVi+CbVO8luqQ32f41",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ueki5/colaboratory/blob/main/playground-019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz | tail -n 1\n",
        "!pip install torchinfo | tail -n 1\n",
        "!pip install japanize-matplotlib | tail -n 1\n",
        "!pip install plotly | tail -n 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljFG6GVDBoaC",
        "outputId": "748cb162-f93e-4f44-89f4-f751cb155f60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed torchviz-0.0.3\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Successfully installed japanize-matplotlib-1.1.3\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import japanize_matplotlib\n",
        "japanize_matplotlib.japanize()\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# 各種定数\n",
        "batch_size = 500 # ミニバッチのサイズ指定\n",
        "num_epochs = 10 # 繰り返し回数\n",
        "lr = 0.01  # 学習率\n",
        "dim_hidden = 128 # 隠れ層の次元数\n",
        "dim_output = 10 # 出力層の次元数\n",
        "num_layer = 2 # レイヤー数\n",
        "use_cuda = True # CUDA使用\n",
        "\n",
        "# デバイスの割り当て\n",
        "if use_cuda:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "# ダウンロード先ディレクトリ名\n",
        "data_root = './data'\n",
        "\n",
        "#######################################################\n",
        "# 予測・正解サンプリング\n",
        "#######################################################\n",
        "# 正解データ付きで、最初のX0個をイメージ表示\n",
        "\n",
        "# テンソル化 ＋ 正規化 ＋ １階テンソル化\n",
        "transform = transforms.Compose([\n",
        "  # データのテンソル化\n",
        "  transforms.ToTensor(),\n",
        "  # データの正規化(Normalize(μ, σ) ⇒ (x - μ) / σ)\n",
        "  transforms.Normalize(0.5, 0.5),\n",
        "  # １階テンソル化\n",
        "  # transforms.Lambda(lambda x: x.view(-1)),\n",
        "])\n",
        "\n",
        "img_set = datasets.CIFAR10(\n",
        "  root = data_root,\n",
        "  train = False,\n",
        "  download = True,\n",
        ")\n",
        "\n",
        "# 最初の要素の取得\n",
        "ROW_NUM = 5\n",
        "COL_NUM = 10\n",
        "fig = plt.figure(figsize=(COL_NUM, ROW_NUM + 1))\n",
        "plt.suptitle('CIFAR10')\n",
        "for i in range(COL_NUM * ROW_NUM):\n",
        "  image, label = img_set[i]\n",
        "  ax = plt.subplot(ROW_NUM, COL_NUM, i + 1)\n",
        "  # イメージ表示\n",
        "  plt.imshow(image, cmap='gray_r')\n",
        "  ax.set_title(f'{label}')\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "\n",
        "# # テンソル化 ＋ 正規化 ＋ １階テンソル化\n",
        "# transform = transforms.Compose([\n",
        "#   # データのテンソル化\n",
        "#   transforms.ToTensor(),\n",
        "#   # データの正規化(Normalize(μ, σ) ⇒ (x - μ) / σ)\n",
        "#   transforms.Normalize(0.5, 0.5),\n",
        "#   # １階テンソル化\n",
        "#   transforms.Lambda(lambda x: x.view(-1)),\n",
        "# ])\n",
        "\n",
        "# # 訓練データ\n",
        "# train_set = datasets.CIFAR10(\n",
        "#   root=data_root,\n",
        "#   train=True,\n",
        "#   download=True,\n",
        "#   transform=transform\n",
        "# )\n",
        "\n",
        "# # 検証データ\n",
        "# test_set = datasets.CIFAR10(\n",
        "#   root=data_root,\n",
        "#   train=False,\n",
        "#   download=True,\n",
        "#   transform=transform\n",
        "# )\n",
        "# # 変換結果の確認\n",
        "# image, label = train_set[0]\n",
        "# print('訓練データの型：', type(image))\n",
        "# print('訓練データのshape：', image.shape)\n",
        "# print('最小値：', image.data.min())\n",
        "# print('最大値：', image.data.max())\n",
        "\n",
        "# image, label = test_set[0]\n",
        "# print('検証データの型：', type(image))\n",
        "# print('検証データのshape：', image.shape)\n",
        "# print('最小値：', image.data.min())\n",
        "# print('最大値：', image.data.max())\n",
        "\n",
        "# # 訓練用データローダー\n",
        "# # 訓練用なので、シャッフルをかける\n",
        "# train_loader = DataLoader(\n",
        "#     train_set,\n",
        "#     batch_size = batch_size,\n",
        "#     shuffle = True,\n",
        "# )\n",
        "\n",
        "# # 検証用データローダー\n",
        "# # 検証用にシャッフルは不要\n",
        "# test_loader = DataLoader(\n",
        "#     test_set,\n",
        "#     batch_size = batch_size,\n",
        "#     shuffle = False,\n",
        "# )\n",
        "\n",
        "# # 何組のデータが取得できるか？\n",
        "# num_data = len(train_loader)\n",
        "# num_data_test = len(test_loader)\n",
        "# print(f'num_data:{num_data},num_data_test:{num_data_test}')\n",
        "\n",
        "# # データローダーから最初の１セットを取得する\n",
        "# inputs, labels = next(iter(train_loader))\n",
        "# print('inputs.shape:', inputs.shape)\n",
        "# print('labels.shape:', labels.shape)\n",
        "\n",
        "# # 入力次元数\n",
        "# n_input = inputs.shape[1] # 0:ミニバッチサイズ、1:入力次元数\n",
        "\n",
        "# # 出力次元数\n",
        "# n_output = dim_output\n",
        "\n",
        "# # 隠れ層の次元数\n",
        "# n_hidden = dim_hidden\n",
        "\n",
        "# # 結果確認\n",
        "# print(f'n_input: {n_input}, n_hidden: {n_hidden}, n_output: {n_output}')\n",
        "\n",
        "# ###################################################\n",
        "# # 関数定義\n",
        "# ###################################################\n",
        "# class Net(torch.nn.Module):\n",
        "#     def __init__(self, n_input, n_output, n_hidden, n_layer):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # 予測関数\n",
        "#         layers = []\n",
        "#         for i in range(n_layer):\n",
        "#             _n_input = n_hidden\n",
        "#             _n_output = n_hidden\n",
        "#             if i == 0:\n",
        "#                 _n_input = n_input\n",
        "#             if i == n_layer - 1:\n",
        "#                 _n_output = n_output\n",
        "#             print(f'in={_n_input}, out={_n_output}')\n",
        "#             layers.append(torch.nn.Linear(_n_input, _n_output))\n",
        "#         self.layers = torch.nn.ModuleList(layers)\n",
        "\n",
        "#         # ReLU関数\n",
        "#         self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "#     # 予測関数\n",
        "#     def forward(self, inputs):\n",
        "#       x = self.layers[0](inputs)\n",
        "#       for layer in self.layers[1:]:\n",
        "#         x = self.relu(x)\n",
        "#         x = layer(x)\n",
        "#       return x\n",
        "\n",
        "# start_time = datetime.now()\n",
        "# # 予測計算オブジェクトの作成\n",
        "# net = Net(n_input, n_output, n_hidden, num_layer).to(device)\n",
        "\n",
        "# # 損失関数\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# # 最適化関数\n",
        "# optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# # 記録用配列初期化()\n",
        "# history = np.zeros((0, 5))\n",
        "\n",
        "# # 学習＆検証\n",
        "# for epoch in range(num_epochs):\n",
        "#   ###########################\n",
        "#   # 履歴保存用変数\n",
        "#   ###########################\n",
        "#   # １エポック当たりの正解データ累積件数\n",
        "#   n_cum_train_acc = 0\n",
        "#   n_cum_test_acc = 0\n",
        "#   # １エポック当たりの累積損失（平均化前）\n",
        "#   cum_train_loss = 0\n",
        "#   cum_test_loss = 0\n",
        "#   # １エポック当たりのデータ累積件数\n",
        "#   n_cum_train = 0\n",
        "#   n_cum_test = 0\n",
        "\n",
        "#   ###########################\n",
        "#   # 学習\n",
        "#   ###########################\n",
        "#   for inputs, labels in tqdm(train_loader):\n",
        "#     # 入力データをデバイスへ転送\n",
        "#     inputs = inputs.to(device)\n",
        "#     labels = labels.to(device)\n",
        "\n",
        "#     # １バッチ当たりのデータ件数\n",
        "#     train_batch_size = len(labels)\n",
        "#     # １エポック当たりのデータ累積件数\n",
        "#     n_cum_train += train_batch_size\n",
        "\n",
        "#     # 予測計算\n",
        "#     outputs = net(inputs)\n",
        "\n",
        "#     # 損失計算\n",
        "#     loss = criterion(outputs, labels)\n",
        "\n",
        "#     # 勾配計算\n",
        "#     loss.backward()\n",
        "\n",
        "#     # パラメータ調整\n",
        "#     optimizer.step()\n",
        "\n",
        "#     # 勾配クリア\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     # 損失の保存（スカラー値の取得）\n",
        "#     train_loss = loss.item()\n",
        "\n",
        "#     # 予測データ（0, 1, ... ,9）計算\n",
        "#     predicted = torch.max(outputs, 1)\n",
        "\n",
        "#     # 正解数の保存\n",
        "#     n_train_acc = (predicted.indices == labels).sum()\n",
        "\n",
        "#     # １エポック当たりの累積正解数\n",
        "#     n_cum_train_acc += n_train_acc\n",
        "\n",
        "#     # １エポック当たりの累積損失（平均化前）\n",
        "#     cum_train_loss += train_loss * train_batch_size\n",
        "\n",
        "#   ###########################\n",
        "#   # 検証\n",
        "#   ###########################\n",
        "#   for inputs_test, labels_test in tqdm(test_loader):\n",
        "#     # 入力データをデバイスへ転送\n",
        "#     inputs_test = inputs_test.to(device)\n",
        "#     labels_test = labels_test.to(device)\n",
        "#     # １バッチ当たりのデータ件数\n",
        "#     test_batch_size = len(labels_test)\n",
        "#     # １エポック当たりのデータ累積件数\n",
        "#     n_cum_test += test_batch_size\n",
        "\n",
        "#     # 予測計算\n",
        "#     outputs_test = net(inputs_test)\n",
        "\n",
        "#     # 損失計算\n",
        "#     loss = criterion(outputs_test, labels_test)\n",
        "\n",
        "#     # 損失の保存（スカラー値の取得）\n",
        "#     test_loss = loss.item()\n",
        "\n",
        "#     # 予測データ（0, 1, ... ,9）計算\n",
        "#     predicted_test = torch.max(outputs_test, 1)\n",
        "\n",
        "#     # 正解数の保存\n",
        "#     n_test_acc = (predicted_test.indices == labels_test).sum()\n",
        "\n",
        "#     # １エポック当たりの累積正解数\n",
        "#     n_cum_test_acc += n_test_acc\n",
        "\n",
        "#     # １エポック当たりの累積損失（平均化前）\n",
        "#     cum_test_loss += test_loss * test_batch_size\n",
        "\n",
        "#   ###########################\n",
        "#   # 履歴を保存\n",
        "#   ###########################\n",
        "#   item = np.array([\n",
        "#     epoch,\n",
        "#     cum_train_loss / n_cum_train,\n",
        "#     (n_cum_train_acc / n_cum_train).cpu(),\n",
        "#     cum_test_loss / n_cum_test,\n",
        "#     (n_cum_test_acc / n_cum_test).cpu(),\n",
        "#   ])\n",
        "#   history = np.vstack((history, item))\n",
        "\n",
        "# end_time = datetime.now()\n",
        "# td = end_time - start_time\n",
        "# total_us = td.seconds * 1_000_000 + td.microseconds\n",
        "# h = total_us // (3600 * 1_000_000)\n",
        "# m = (total_us // (60 * 1_000_000)) % 60\n",
        "# s = (total_us // 1_000_000) % 60\n",
        "# us = total_us % 1_000_000\n",
        "# print(f'開始　　{start_time.strftime(\"%H:%M:%S.%f\")}')\n",
        "# print(f'終了　　{end_time.strftime(\"%H:%M:%S.%f\")}')\n",
        "# print(f'処理　　{h:02}:{m:02}:{s:02}.{us:06}')\n",
        "\n",
        "# ###############################################\n",
        "# # 結果表示\n",
        "# ###############################################\n",
        "# # 損失と精度の確認\n",
        "# print(f'初期状態: 損失: {history[ 0, 3]:.5f} 精度: {history[ 0, 4]:.5f}')\n",
        "# print(f'最終状態: 損失: {history[-1, 3]:.5f} 精度: {history[-1, 4]:.5f}')\n",
        "\n",
        "# # 学習曲線グラフ（損失）\n",
        "# plt.plot(history[:, 0], history[:, 1], 'b', label='学習')\n",
        "# plt.plot(history[:, 0], history[:, 3], 'k', label='検証')\n",
        "# plt.xlabel('回数')\n",
        "# plt.ylabel('損失')\n",
        "# plt.title('学習曲線グラフ（損失）')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# # 学習曲線グラフ（精度）\n",
        "# plt.plot(history[:, 0], history[:, 2], 'b', label='学習')\n",
        "# plt.plot(history[:, 0], history[:, 4], 'k', label='検証')\n",
        "# plt.xlabel('回数')\n",
        "# plt.ylabel('精度')\n",
        "# plt.title('学習曲線グラフ（精度）')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# #######################################################\n",
        "# # 予測・正解サンプリング\n",
        "# #######################################################\n",
        "# # 正解データ付きで、最初のX0個をイメージ表示\n",
        "# img_set = datasets.MNIST(\n",
        "#   root = data_root,\n",
        "#   train = False,\n",
        "#   download = True,\n",
        "# )\n",
        "# test_set = datasets.MNIST(\n",
        "#   root=data_root,\n",
        "#   train=False,\n",
        "#   download=True,\n",
        "#   transform=transform,\n",
        "# )\n",
        "# test_loader = DataLoader(\n",
        "#     test_set,\n",
        "#     batch_size = batch_size,\n",
        "#     shuffle = False,\n",
        "# )\n",
        "# inputs_test, labels_test = next(iter(test_loader))\n",
        "# # 入力データをデバイスへ転送\n",
        "# inputs_test = inputs_test.to(device)\n",
        "# labels_test = labels_test.to(device)\n",
        "# # 予測計算\n",
        "# outputs_test = net(inputs_test)\n",
        "# # 損失計算\n",
        "# loss = criterion(outputs_test, labels_test)\n",
        "# # 損失の保存（スカラー値の取得）\n",
        "# test_loss = loss.item()\n",
        "# # 予測データ（0, 1, ... ,9）計算\n",
        "# predicted_test = torch.max(outputs_test, 1)\n",
        "# print('predicted_test.indices:', len(predicted_test.indices))\n",
        "\n",
        "# # 最初の要素の取得\n",
        "# ROW_NUM = 5\n",
        "# COL_NUM = 10\n",
        "# fig = plt.figure(figsize=(COL_NUM, ROW_NUM + 1))\n",
        "# plt.suptitle('予測:正解')\n",
        "# for i in range(COL_NUM * ROW_NUM):\n",
        "#   image, label = img_set[i]\n",
        "#   ax = plt.subplot(ROW_NUM, COL_NUM, i + 1)\n",
        "#   # イメージ表示\n",
        "#   plt.imshow(image, cmap='gray_r')\n",
        "#   ax.set_title(f'{predicted_test.indices[i]}:{label}')\n",
        "#   ax.get_xaxis().set_visible(False)\n",
        "#   ax.get_yaxis().set_visible(False)\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "SQjHJyKHuPFt",
        "outputId": "91e9c711-885e-4c6b-f125-5e47d8dc03d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (48000x32 and 784x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-563143848.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# 予測計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0moutputs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;31m# 損失計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-166906417.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# 予測関数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (48000x32 and 784x128)"
          ]
        }
      ]
    }
  ]
}