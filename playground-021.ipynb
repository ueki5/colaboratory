{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSn5fyo8CVbwvZ5KZIAjh7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ueki5/colaboratory/blob/main/playground-021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz | tail -n 1\n",
        "!pip install torchinfo | tail -n 1\n",
        "!pip install japanize-matplotlib | tail -n 1\n",
        "!pip install plotly | tail -n 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljFG6GVDBoaC",
        "outputId": "99dd53f7-64bb-4cda-97eb-f720f85935e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchviz) (3.0.3)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->japanize-matplotlib) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import japanize_matplotlib\n",
        "japanize_matplotlib.japanize()\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "japanize_matplotlib.japanize()\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchviz import make_dot\n",
        "\n",
        "\n",
        "def tensor_to_pil(tensor, *, unnormalize=None, clip=True, convert_mode=None):\n",
        "    \"\"\"\n",
        "    PyTorch Tensor -> PIL.Image.Image\n",
        "\n",
        "    Args:\n",
        "        tensor: torch.Tensor\n",
        "            - shape can be (C,H,W), (H,W), (N,C,H,W) (then first item used), dtype float or uint8, on cpu or cuda\n",
        "        unnormalize: callable or tuple(mean, std) or None\n",
        "            - If tensor is normalized (e.g. ImageNet), pass (mean, std) to unnormalize each channel:\n",
        "              unnormalize = (mean, std) where mean/std are sequences of length C.\n",
        "            - Or pass a callable that maps tensor -> tensor (e.g. custom unnormalizer).\n",
        "        clip: bool\n",
        "            - Whether to clip final numeric values to valid range before converting to uint8.\n",
        "        convert_mode: str or None\n",
        "            - If you want to force PIL mode, e.g. 'RGB', 'L'. If None, infer from array shape.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image\n",
        "    \"\"\"\n",
        "    # 1.GPU → .cpu()\n",
        "    # 2.計算グラフ → .detach()\n",
        "    # 1) detach & cpu\n",
        "    if isinstance(tensor, torch.Tensor):\n",
        "        t = tensor.detach()\n",
        "        if t.device.type != 'cpu':\n",
        "            t = t.cpu()\n",
        "    else:\n",
        "        raise TypeError(\"Expected torch.Tensor\")\n",
        "\n",
        "    # 3.バッチ → tensor[0] or iterate\n",
        "    # 4.(C,H,W) → (H,W,C)（permute(1,2,0)）\n",
        "    # 2) if batch, take first element\n",
        "    if t.ndim == 4:  # (N, C, H, W)\n",
        "        t = t[0]\n",
        "\n",
        "    # 3) squeeze channel if redundant\n",
        "    if t.ndim == 3:\n",
        "        c, h, w = t.shape\n",
        "    elif t.ndim == 2:\n",
        "        # (H, W) grayscale\n",
        "        h, w = t.shape\n",
        "        c = None\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported tensor shape: {t.shape}\")\n",
        "\n",
        "    # 5.正規化を戻す（必要なら）\n",
        "    # 4) optionally unnormalize (mean/std)\n",
        "    if unnormalize is not None:\n",
        "        if callable(unnormalize):\n",
        "            t = unnormalize(t)\n",
        "        else:\n",
        "            # assume (mean, std)\n",
        "            mean, std = unnormalize\n",
        "            mean = torch.tensor(mean, dtype=t.dtype, device=t.device)\n",
        "            std = torch.tensor(std, dtype=t.dtype, device=t.device)\n",
        "            # if single channel mean/std, expand\n",
        "            if mean.ndim == 0 or mean.shape[0] == 1:\n",
        "                t = (t * std) + mean\n",
        "            else:\n",
        "                # (C,H,W) -> broadcast over H,W\n",
        "                t = t * std[:, None, None] + mean[:, None, None]\n",
        "\n",
        "    # 5) convert to numpy-friendly layout (H, W, C) or (H, W)\n",
        "    if t.ndim == 3:\n",
        "        # PyTorch: (C, H, W) -> NumPy/PIL: (H, W, C)\n",
        "        arr = t.permute(1, 2, 0).numpy()\n",
        "    else:\n",
        "        arr = t.numpy()\n",
        "\n",
        "    # 6.dtype とレンジを uint8 / 0..255 に整える\n",
        "    # 6) decide range & dtype\n",
        "    # Common cases:\n",
        "    #  - float in [0,1]: multiply by 255 -> uint8\n",
        "    #  - float in [0,255]: just clip/round -> uint8\n",
        "    #  - uint8 already: pass through\n",
        "    if arr.dtype == np.uint8:\n",
        "        out = arr\n",
        "    else:\n",
        "        # float or other int types\n",
        "        # heuristics: if float and max <= 1.0 -> treat as [0,1]\n",
        "        if np.issubdtype(arr.dtype, np.floating):\n",
        "            m = arr.max()\n",
        "            M = arr.min()\n",
        "            if m <= 1.0 + 1e-8 and M >= -1e-8:\n",
        "                arr = arr * 255.0\n",
        "            # sometimes tensors are in range [-1,1] (e.g. tanh outputs)\n",
        "            elif m <= 1.0 and M < 0:\n",
        "                arr = (arr + 1.0) / 2.0 * 255.0\n",
        "            # otherwise assume already in 0..255-ish\n",
        "        # cast to float64 for safe rounding\n",
        "        arr = np.round(arr).astype(np.int64)\n",
        "        if clip:\n",
        "            arr = np.clip(arr, 0, 255)\n",
        "        out = arr.astype(np.uint8)\n",
        "\n",
        "    # 7) choose PIL mode\n",
        "    if convert_mode is None:\n",
        "        if out.ndim == 2:\n",
        "            mode = 'L'  # grayscale\n",
        "        elif out.shape[2] == 1:\n",
        "            out = out[:, :, 0]\n",
        "            mode = 'L'\n",
        "        elif out.shape[2] == 3:\n",
        "            mode = 'RGB'\n",
        "        elif out.shape[2] == 4:\n",
        "            mode = 'RGBA'\n",
        "        else:\n",
        "            # fallback: treat as RGB by truncation/padding\n",
        "            if out.shape[2] > 4:\n",
        "                out = out[:, :, :3]\n",
        "                mode = 'RGB'\n",
        "            else:\n",
        "                # pad channels with zeros\n",
        "                pad = 3 - out.shape[2]\n",
        "                out = np.pad(out, ((0,0),(0,0),(0,pad)), mode='constant')\n",
        "                mode = 'RGB'\n",
        "    else:\n",
        "        mode = convert_mode\n",
        "\n",
        "    # 7.Image.fromarray(numpy_array) で PIL に変換\n",
        "    # 8) to PIL\n",
        "    # pil = Image.fromarray(out, mode=mode)\n",
        "    pil = Image.fromarray(out)\n",
        "    return pil\n",
        "\n",
        "# Shortcut: use torchvision (works for many common cases)\n",
        "def tensor_to_pil_torchvision(tensor):\n",
        "    # torchvision.transforms.functional.to_pil_image handles many cases\n",
        "    if tensor.device.type != 'cpu':\n",
        "        tensor = tensor.cpu()\n",
        "    if tensor.ndim == 4:\n",
        "        tensor = tensor[0]\n",
        "    return TF.to_pil_image(tensor)\n",
        "\n",
        "# 畳み込み\n",
        "def conv2d(data: torch.Tensor) -> torch.Tensor:\n",
        "  (I, J, K) = data.shape\n",
        "  print(f\"{I},{J},{K}\")\n",
        "  ary = np.zeros((I, J - 2, K - 2), dtype = np.float32)\n",
        "  for i in range(I):\n",
        "    for j in range(1, J - 1):\n",
        "      for k in range(1, K - 1):\n",
        "        a = np.array([\n",
        "          [data[i][j-1][k-1].item(), data[i][ j ][k-1].item(), data[i][j+1][k-1].item()],\n",
        "          [data[i][j-1][ k ].item(), data[i][ j ][ k ].item(), data[i][j+1][ k ].item()],\n",
        "          [data[i][j-1][k+1].item(), data[i][ j ][k+1].item(), data[i][j+1][k+1].item()],\n",
        "        ], dtype=np.float32)\n",
        "        b = np.array([\n",
        "          [1, 0, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 0, 1],\n",
        "        ], dtype=np.float32)\n",
        "        ary[i][j-1][k-1] = np.sum(a * b) / sum(b.flatten())\n",
        "  ary_tensor = torch.from_numpy(ary)\n",
        "  return ary_tensor\n",
        "\n",
        "# ダウンロード先ディレクトリ名\n",
        "data_root = './data'\n",
        "\n",
        "# テンソル化 ＋ 正規化\n",
        "transform = transforms.Compose([\n",
        "  # データのテンソル化\n",
        "  transforms.ToTensor(),\n",
        "  # データの正規化(Normalize(μ, σ) ⇒ (x - μ) / σ)\n",
        "  transforms.Normalize(0.5, 0.5),\n",
        "])\n",
        "# 検証データ\n",
        "test_set = datasets.CIFAR10(\n",
        "  root=data_root,\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=transform\n",
        ")\n",
        "\n",
        "data, label = test_set[0]\n",
        "\n",
        "# 画像変換\n",
        "tensor = []\n",
        "image = []\n",
        "tensor.append(data)\n",
        "image.append(tensor_to_pil(data))\n",
        "for i in range(3):\n",
        "  tensor.append(conv2d(tensor[i]))\n",
        "  image.append(tensor_to_pil(tensor[i+1]))\n",
        "\n",
        "# イメージ比較\n",
        "ROW_NUM = 1\n",
        "COL_NUM = 3\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "# plt.suptitle('イメージ比較')\n",
        "for i in range(3):\n",
        "  ax = plt.subplot(1, 2, i)\n",
        "  plt.imshow(image[i])\n",
        "  ax.set_title(f'{i}')\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "SQjHJyKHuPFt",
        "outputId": "7c765ff8-3050-49b8-b447-8f657ef98949"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3,32,32\n",
            "3,30,30\n",
            "3,28,28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}